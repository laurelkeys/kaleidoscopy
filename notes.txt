We use global variables for simplicity [[IdentifierStr and NumVal]], but this is not the best choice for a real language implementation :).

In Kaleidoscope, functions are typed with just a count of their arguments.
Since all values are double precision floating point, the type of each argument doesn’t need to be stored anywhere.
In a more aggressive and realistic language, the “ExprAST” class would probably have a type field.

Because errors can occur, the parser needs a way to indicate that they happened: in our parser, we return null on an error.

For the basic form of Kaleidoscope, we will only support 4 binary operators.

To start, an expression is a primary expression potentially followed by a sequence of [binop,primaryexpr] pairs.

The Builder object is a helper object that makes it easy to generate LLVM instructions.
Instances of the IRBuilder class template keep track of the current place to insert instructions and has methods to create new instructions.

TheModule is an LLVM construct that contains functions and global variables.
In many ways, it is the top-level structure that the LLVM IR uses to contain code.
It will own the memory for all of the IR that we generate, which is why the codegen() method returns a raw Value*, rather than a unique_ptr<Value>.

Code generation for function calls is quite straightforward with LLVM.
The code above initially does a function name lookup in the LLVM Module’s symbol table.
Recall that the LLVM Module is the container that holds the functions we are JIT’ing.
By giving each function the same name as what the user specifies, we can use the LLVM symbol table to resolve function names for us.

Unfortunately, no amount of local analysis will be able to detect and correct this.
This requires two transformations: reassociation of expressions (to make the add’s lexically identical)
and Common Subexpression Elimination (CSE) to delete the redundant add instruction.
Fortunately, LLVM provides a broad range of optimizations that you can use, in the form of “passes”.

LLVM provides a wide variety of optimizations that can be used in certain circumstances.
Some documentation about the various passes is available [[at http://llvm.org/docs/Passes.html]], but it isn’t very complete.
Another good source of ideas can come from looking at the passes that Clang runs to get started.
[[The ones recommended were: InstructionCombiningPass, ReassociatePass, GVNPass and CFGSimplificationPass.]]

In Kaleidoscope, every construct is an expression: there are no statements.
As such, the if/then/else expression needs to return a value like any other.
Since we’re using a mostly functional form, we’ll have it evaluate its conditional, then return the ‘then’ or ‘else’ value based on how the condition was resolved.
This is very similar to the C “?:” expression

To visualize the control flow graph, you can use a nifty feature of the LLVM ‘opt’ tool.
If you put this LLVM IR into “t.ll” and run “llvm-as < t.ll | opt -analyze -view-cfg”, a window will pop up and you’ll see this graph: [[...]].
Another way to get this is to call “F->viewCFG()” or “F->viewCFGOnly()” (where F is a “Function*”) either by inserting actual calls into the code and recompiling or by calling these in the debugger.
LLVM has many nice features for visualizing various graphs.

In practice, there are two sorts of values that float around in code written for your average imperative programming language that might need Phi nodes:
Code that involves user variables: x = 1; x = x + 1;
Values that are implicit in the structure of your AST, such as the Phi node in this case.
In Chapter 7 of this tutorial (“mutable variables”), we’ll talk about #1 in depth.
For now, just believe me that you don’t need SSA construction to handle this case.
For #2, you have the choice of using the techniques that we will describe for #1, or you can insert Phi nodes directly, if convenient.
In this case, it is really easy to generate the Phi node, so we choose to do it directly.

One interesting (and very important) aspect of the LLVM IR is that it requires all basic blocks to be “terminated” with a control flow instruction such as return or branch.
This means that all control flow, including fall throughs must be made explicit in the LLVM IR.
If you violate this rule, the verifier will emit an error.

The ‘trick’ here is that while LLVM does require all register values to be in SSA form, it does not require (or permit) memory objects to be in SSA form.
This differs from some other compiler systems, which do try to version memory objects.
In LLVM, instead of encoding dataflow analysis of memory into the LLVM IR, it is handled with Analysis Passes which are computed on demand.
With this in mind, the high-level idea is that we want to make a stack variable (which lives in memory, because it is on the stack) for each mutable object in a function.

In LLVM, all memory accesses are explicit with load/store instructions, and it is carefully designed not to have (or need) an “address-of” operator.

define i32 @example() {
entry:
  %X = alloca i32           ; type of %X is i32*.
  ...
  %tmp = load i32* %X       ; load the stack value %X from the stack.
  %tmp2 = add i32 %tmp, 1   ; increment it
  store i32 %tmp2, i32* %X  ; store it back
  ...

We could rewrite the example to use the alloca technique to avoid using a PHI node:
#######################
## with a PHI node ####
#######################
@G = weak global i32 0   ; type of @G is i32*
@H = weak global i32 0   ; type of @H is i32*

define i32 @test(i1 %Condition) {
entry:
  br i1 %Condition, label %cond_true, label %cond_false

cond_true:
  %X.0 = load i32* @G
  br label %cond_next

cond_false:
  %X.1 = load i32* @H
  br label %cond_next

cond_next:
  %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]
  ret i32 %X.2
}
#######################
## with alloca ########
#######################
@G = weak global i32 0   ; type of @G is i32*
@H = weak global i32 0   ; type of @H is i32*

define i32 @test(i1 %Condition) {
entry:
  %X = alloca i32           ; type of %X is i32*.
  br i1 %Condition, label %cond_true, label %cond_false

cond_true:
  %X.0 = load i32* @G
  store i32 %X.0, i32* %X   ; Update X
  br label %cond_next

cond_false:
  %X.1 = load i32* @H
  store i32 %X.1, i32* %X   ; Update X
  br label %cond_next

cond_next:
  %X.2 = load i32* %X       ; Read X
  ret i32 %X.2
}
#######################

With this, we have discovered a way to handle arbitrary mutable variables without the need to create Phi nodes at all:
1. Each mutable variable becomes a stack allocation.
2. Each read of the variable becomes a load from the stack.
3. Each update of the variable becomes a store to the stack.
4. Taking the address of a variable just uses the stack address directly.

Fortunately for us, the LLVM optimizer has a highly-tuned optimization pass named “mem2reg” that handles this case, promoting allocas like this into SSA registers, inserting Phi nodes as appropriate.