We use global variables for simplicity [[IdentifierStr and NumVal]], but this is not the best choice for a real language implementation :).

In Kaleidoscope, functions are typed with just a count of their arguments.
Since all values are double precision floating point, the type of each argument doesn’t need to be stored anywhere.
In a more aggressive and realistic language, the “ExprAST” class would probably have a type field.

Because errors can occur, the parser needs a way to indicate that they happened: in our parser, we return null on an error.

For the basic form of Kaleidoscope, we will only support 4 binary operators.

To start, an expression is a primary expression potentially followed by a sequence of [binop,primaryexpr] pairs.

The Builder object is a helper object that makes it easy to generate LLVM instructions.
Instances of the IRBuilder class template keep track of the current place to insert instructions and has methods to create new instructions.

TheModule is an LLVM construct that contains functions and global variables.
In many ways, it is the top-level structure that the LLVM IR uses to contain code.
It will own the memory for all of the IR that we generate, which is why the codegen() method returns a raw Value*, rather than a unique_ptr<Value>.

Code generation for function calls is quite straightforward with LLVM.
The code above initially does a function name lookup in the LLVM Module’s symbol table.
Recall that the LLVM Module is the container that holds the functions we are JIT’ing.
By giving each function the same name as what the user specifies, we can use the LLVM symbol table to resolve function names for us.

Unfortunately, no amount of local analysis will be able to detect and correct this.
This requires two transformations: reassociation of expressions (to make the add’s lexically identical)
and Common Subexpression Elimination (CSE) to delete the redundant add instruction.
Fortunately, LLVM provides a broad range of optimizations that you can use, in the form of “passes”.

LLVM provides a wide variety of optimizations that can be used in certain circumstances.
Some documentation about the various passes is available [[at http://llvm.org/docs/Passes.html]], but it isn’t very complete.
Another good source of ideas can come from looking at the passes that Clang runs to get started.
[[The ones recommended were: InstructionCombiningPass, ReassociatePass, GVNPass and CFGSimplificationPass.]]

In Kaleidoscope, every construct is an expression: there are no statements.
As such, the if/then/else expression needs to return a value like any other.
Since we’re using a mostly functional form, we’ll have it evaluate its conditional, then return the ‘then’ or ‘else’ value based on how the condition was resolved.
This is very similar to the C “?:” expression

To visualize the control flow graph, you can use a nifty feature of the LLVM ‘opt’ tool.
If you put this LLVM IR into “t.ll” and run “llvm-as < t.ll | opt -analyze -view-cfg”, a window will pop up and you’ll see this graph: [[...]].
Another way to get this is to call “F->viewCFG()” or “F->viewCFGOnly()” (where F is a “Function*”) either by inserting actual calls into the code and recompiling or by calling these in the debugger.
LLVM has many nice features for visualizing various graphs.

In practice, there are two sorts of values that float around in code written for your average imperative programming language that might need Phi nodes:
Code that involves user variables: x = 1; x = x + 1;
Values that are implicit in the structure of your AST, such as the Phi node in this case.
In Chapter 7 of this tutorial (“mutable variables”), we’ll talk about #1 in depth.
For now, just believe me that you don’t need SSA construction to handle this case.
For #2, you have the choice of using the techniques that we will describe for #1, or you can insert Phi nodes directly, if convenient.
In this case, it is really easy to generate the Phi node, so we choose to do it directly.

One interesting (and very important) aspect of the LLVM IR is that it requires all basic blocks to be “terminated” with a control flow instruction such as return or branch.
This means that all control flow, including fall throughs must be made explicit in the LLVM IR.
If you violate this rule, the verifier will emit an error.

The ‘trick’ here is that while LLVM does require all register values to be in SSA form, it does not require (or permit) memory objects to be in SSA form.
In LLVM, instead of encoding dataflow analysis of memory into the LLVM IR, it is handled with Analysis Passes which are computed on demand.
With this in mind, the high-level idea is that we want to make a stack variable (which lives in memory, because it is on the stack) for each mutable object in a function.

In LLVM, all memory accesses are explicit with load/store instructions, and it is carefully designed not to have (or need) an “address-of” operator.

[[...]] the type of the @G/@H global variables is actually “i32*” even though the variable is defined as “i32”.
What this means is that @G defines space for an i32 in the global data area, but its name actually refers to the address for that space.
Stack variables work the same way, except that instead of being declared with global variable definitions, they are declared with the LLVM alloca instruction.

To specify the architecture that you want to target, we use a string called a “target triple”. This takes the form <arch><sub>-<vendor>-<sys>-<abi>.
